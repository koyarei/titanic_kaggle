---
title: 'Titanic: Machine Learning from Disaster -- a Kaggle competition entry'
author: "Sizhan Shi"
date: "November 5, 2014"
output: 
    html_document:
        theme: journal
        highlight: kate

---

## Background  
This is an entry for the kaggle machine learning competition [Titanic: Machine Learning from Disaster](https://www.kaggle.com/c/titanic-gettingStarted)

#### Introduction from kaggle's website:  
> The sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.  

> One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.  

> In this challenge, we ask you to complete the analysis of what sorts of people were likely to survive. In particular, we ask you to apply the tools of machine learning to predict which passengers survived the tragedy.  

> This Kaggle "Getting Started" Competition provides an ideal starting place for people who may not have a lot of experience in data science and machine learning. The data is highly structured, and we provide tutorials of increasing complexity for using Excel, Python, pandas in Python, and a Random Forest in Python (see links in the sidebar). We also have links to tutorials using R instead. Please use the forums freely and as much as you like. There is no such thing as a stupid question; we guarantee someone else will be wondering the same thing!  

#### Method:  
1. Load the data  
2. Perform exploratory analysis  
3. Propose hypothesis  
4. Create model reflective of the hypothesis  
5. Submit model prediction result  
6. Repeat steps 3 through 5 until reaching the limit of accuracy improvement  

#### Reference:  
http://trevorstephens.com/post/72920580937/titanic-getting-started-with-r-part-2-the

## Summary  






## Detailed analysis  

#### Load the data
```{r}
train <- read.csv("train.csv")
test <- read.csv("test.csv")
head(train)
## how many observations 
nrow(train)
## how many features
ncol(train)
## how many survived and their percentage
nrow(train[train$Survived==1,]) / nrow(train)
```

About 38.4% of passengers survived. Let's examine survival rate in relationship to passengers' demographic attributes. First, start with age and survival rate.  
```{r, fig.height=4, fig.width=6}

library(ggplot2)
ggplot(train, aes(as.factor(Survived),Age)) + geom_boxplot() + theme_bw()
```  

From a galance, there doens't seem to have a strong difference in age between survivors and victims. Let's look at sex next.  

### Gender Model:  

```{r, fig.height=4, fig.width=6}
table(train$Sex)
table(train$Sex, train$Survived)  
##  seems like females are more likely to sruvive, let's look at the proportions:
prop.table(table(train$Sex, train$Survived), 1)
```  
74% of females survived, versus only 19% for males.  

#### Round 1 submission:  
Assume all females survived, and all males died.  

#### Round 1 submission result:  
This model had an accuracy of 0.76555.  

### Gender, Class, Fare, and Children Model:  


Now let's get back to the ages.  
```{r}
summary(train$Age)
## there are several NAs. Let's clean them up by replacing them with the average age of all passengers  
yesAge <- train[!is.na(train$Age),]
train[is.na(train$Age),"Age"] <- mean(yesAge$Age)
```  
Here, the assumption is that women and children had priority access to lifeboats and life vests. The question is how young should a child be to be considered eligible for that treatment? Let's use a for loop, starting from the age 18, and compare the difference of the survival rates between boys and girls, if we found an age group with almost equal survival rate between both genders, we have found the threadshold.  

**Update:** This finding was later confirmed by my reserach on [Wikipedia](http://en.wikipedia.org/wiki/RMS_Titanic#Survivors_and_victims). According to the table, there were 109 children in total, since we only have a portion of the data (train), we can calculate the ratio of observation data in the training set versus the complete dataset.  

```{r}
rate <- nrow(train) / (nrow(train) + nrow(test))
```

We have about 68% of all observations, meaning the number of children in our training dataset should follow the same ratio.  

```{r}
rate * 109
```

We should have about 74 children in our training dataset. Let's verify that with our 15 thredshold.  

```{r}
nrow(train[train$Age < 15,])

```
78, very close to the number 74. 15 is the correct threshold. According to the table, 100% of children stayed in second class survived. We will apply this learning later.  

**Update over**  

```{r}
for (i in 2:18) {
    ageTemp <- subset(train, Age < i)
    prop.table(table(ageTemp$Sex, ageTemp$Survived),1)
    diff <- prop.table(table(ageTemp$Sex, ageTemp$Survived),1)[1,2]-prop.table(table(ageTemp$Sex, ageTemp$Survived),1)[2,2]
    print(paste("Age", i, "survival diff: ", diff))
}

```
It seems like age 15 is the threashold -- younger than 15, regardless boy or girl, the person was considered a child and given priority; older than 15, the person was considered an adult, a woman or a man, and was treated accordingly. 

```{r}
age15 <- subset(train, Age < 15)
table(age15$Sex, age15$Survived)
prop.table(table(age15$Sex, age15$Survived), 1)
```  
Children younger than 15 had a survival rate higher than 54%, much better than the average of 38%. Let's assign this level to the passengers.  

```{r}
train$Child <- 0
train[train$Age < 15,]$Child <- 1
```

#### Learnings so far:  
1. Females had higher survival rate than men;  
2. Children under the age of 15 were treated equally regardless of gender;  

Now take another look at the categorization of fare and class. First, find the most common fare price.  

```{r}
for (i in 10:100) {
    propotion <- length(train[train$Fare<i,]$Fare) / nrow(train)
    msg <- paste("Fare lower than ", i, " account for ", round(propotion, digits=2))
    print(msg)
}
```

We can categorize fare into one of the following buckets:  
1. < 10, about 38% passengers;  
2. >=10 and <15, about 13% passengers;  
3. >=15 and <24, about 10% passengers;  
4. >=24 and <28, about 11% passengers;  
5. >=28 and <52, about 10% passengers;  
6. >=52 and <78, about 8% passengers;  
7. >=78, the rest of the 10% passengers;  

Now let's assign a new class to the train dataset.  

```{r}
train$Fare2 <- "78+"
train[train$Fare<78 & train$Fare>=52,]$Fare2 <- "52-78"
train[train$Fare<52 & train$Fare>=28,]$Fare2 <- "28-52"
train[train$Fare<28 & train$Fare>=24,]$Fare2 <- "24-28"
train[train$Fare<24 & train$Fare>=15,]$Fare2 <- "15-24"
train[train$Fare<15 & train$Fare>=10,]$Fare2 <- "10-15"
train[train$Fare<10,]$Fare2 <- "<10"

```

Let's look at the distribution of passengers across gender, class, and fare level:  

```{r}
survSumFareClassSex <- aggregate(Survived ~ Fare2 + Pclass + Sex + Child, data=train, length)
colnames(survSumFareClassSex)[5] <- "Total.Size"
```

Now let's look at their survival rate.  

```{r}
survRate <- function(x) {
    round(sum(x) / length(x),digits=2)
    
}
survRateFareClassSex <- aggregate(Survived ~ Fare2 + Pclass + Sex + Child, data=train, survRate)
colnames(survRateFareClassSex)[5] <- "Survival.Rate"
survRateFareClassSexChildFull <- merge(survRateFareClassSex, survSumFareClassSex)
survRateFareClassSexChildFull[order(-survRateFareClassSexChildFull$Survival.Rate, -survRateFareClassSexChildFull$Total.Size),]
## find the females that had low survival rate 
subset(survRateFareClassSexChildFull, Sex=="female" & Survival.Rate < 0.5)
```

#### Round 2 submission:  
Seesm like females in the 3rd class with a fare price 24-28, 28-52, or 25-78 mostly died. Let's use this information to modify the gender model for round 2 submission.  

```{r}
test <- read.csv("test.csv")
## create a column "Survived"
test$Survived <- 0
test[test$Survived == 0 & test$Sex == "female",]$Survived <- 1
test[test$Sex == "female" & test$Fare >= 24 & test$Fare < 78 & test$Pclass==3,]$Survived <- 0

result2 <- subset(test, select=c(PassengerId, Survived))
write.csv(result2, "result2.csv", row.names=FALSE)
```

#### Round 2 submission result 

My score improved. Now we have 0.77990 accuracy, beat the benchmark *Basic Random Forests Model* provied by Kaggle (0.77512).  

Based on this model, let's investigate male survivors; currently all males were assumed dead, but maybe there were characteristics indicative of their survival.  

```{r}
survRateFareClassSexChildMale <- subset(survRateFareClassSexChildFull, Sex=="male")
survRateFareClassSexChildMale[order(-survRateFareClassSexChildMale$Survival.Rate),]
```

It seems like male children, if stayed in a first or second class cabin, all survived.  

```{r}
subset(survRateFareClassSexChildMale, (Pclass==2 | Pclass==1) & Child==1)
```

#### Round 3 submission:  
Assume male children stayed in first or second class survived. This assumption also is reflective of our learning from Wikipedia, that all children from second class survived, regardless of gender.  


```{r}
test <- read.csv("test.csv")
## create a column "Survived"
test$Survived <- 0
test[test$Survived == 0 & test$Sex == "female",]$Survived <- 1
test[test$Sex == "female" & test$Fare >= 24 & test$Fare < 78 & test$Pclass==3,]$Survived <- 0
test[test$Sex == "male" & (test$Pclass == 1 | test$Pclass == 2) & test$Age < 15 & !is.na(test$Age),]$Survived <- 1

result3 <- subset(test, select=c(PassengerId, Survived))
write.csv(result3, "result3.csv", row.names=FALSE)
```

#### Round 3 submission result:  
Score improved by 0.00478, now accuracy at 0.78469, also beat the *Gender, Price, and Class Based Model* provided by Kaggle (0.77990).  

### Decision Tree Model:  

```{r}
##install.packages("rpart")
library(rpart)
fit <- rpart(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked, data=train, method="class")
##install.packages('rattle')
##install.packages('rpart.plot')
##install.packages('RColorBrewer')
library(rattle)
library(rpart.plot)
library(RColorBrewer)
fancyRpartPlot(fit)
Prediction <- predict(fit, test, type = "class")
result4 <- data.frame(PassengerId = test$PassengerId, Survived = Prediction)
write.csv(result4, "result4.csv", row.names=F)
```

### Round 4 submission result:  
Score did not improve, accuracy from rpart decision tree model is at 0.77033.  

Now let's change some settings within the rpart function via rpart.control.  

```{r, fig.height=4, fig.width=6}
fit <- rpart(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked, data=train,
             method="class", control=rpart.control(minisplit=5, cp=0))
fancyRpartPlot(fit)
Prediction <- predict(fit, test, type = "class")
result5 <- data.frame(PassengerId = test$PassengerId, Survived = Prediction)
write.csv(result5, "result5.csv", row.names=F)

```

### Round 5 submission result:  
Score did not improve; rather, the accuracy dropped to 0.73684, worse than the simple gender model.  

## Feature Engineering  

First, create a combined data frame with both training and test data.  
```{r}
test <- read.csv("test.csv")
train <- read.csv("train.csv")
test$Survived <- NA
combi <- rbind(train, test)
```

Make each passenger's title a factor.  

```{r}
##install.packages("stringr")
library(stringr)
combi$Name <- as.character(combi$Name)

cleanTitle <- function(x) {
    nameSplit <- str_trim(strsplit(x, ",")[[1]][2])
    nameSplitClean <- strsplit(nameSplit, "\\.")[[1]][1]
    nameSplitClean
}

combi$Title <- sapply(combi$Name, cleanTitle)
```

Summarize the titles:  
+ Capt: honorific addressed to someone served in military with Captain ranking
+ Col: someone served in military with Colonel ranking (higher than Captain)  
+ Don: Spanish honorific equivelant to "Mister"  
+ Dona: Spanish honorific equivelant to "Miss"  
+ Dr: Someone with a doctorate-level degree (likely wealthy)    
+ Jonkheer: Dutch honorific of nobility, equivelant to "Master"  
+ Lady: a woman of nobility  
+ Major: someone served in military, higher than Captain but lower than Colonel  
+ Master: a young unmarried boy  
+ Miss: a young unmarried woman  
+ Mlle: french honorific equivelant to "Miss"  
+ Mme: french honorific equivelant to "Mrs"  
+ Ms: equivelant to Mrs  
+ Rev: a Chiristian clergy or minister  
+ Sir: a knight  
+ Countess:  a woman of nobility  
 
 Combine duplicate titles:  
 
```{r}
combi[combi$Title %in% c("Capt", "Col", "Major", "Sir", "Dr"),]$Title <- "Sir"
combi[combi$Title %in% c("Don", "Mr"),]$Title <- "Mr"
combi[combi$Title %in% c("Dona", "Miss", "Mlle"),]$Title <- "Miss"
combi[combi$Title %in% c("Mrs", "Mme", "Ms"),]$Title <- "Mrs"
combi[combi$Title %in% c("Master", "Jonkheer"),]$Title <- "Master"
combi[combi$Title %in% c("Lady", "the Countess"),]$Title <- "Lady"
```

Now let's add a factor indicating the family size the passenger was traveling with, also identifying their family name.   

```{r}
combi$Fam.Size <- combi$Parch + combi$SibSp + 1
## create a new factor that is the traveler's last name.
combi$Last.Name <- as.character(combi$Name)

getLast.Name <- function(x) {
    Last.Name <- str_trim(strsplit(x, ",")[[1]][1])
    Last.Name
}

combi$Last.Name <- sapply(combi$Name, getLast.Name)
```

Create a new factor called "Fam.Id"" combining family name and family size.  

```{r}
combi$Fam.Id <- paste0(combi$Fam.Size, combi$Last.Name)
Fam.Id <- data.frame(table(combi$Fam.Id))
Fam.Id <- Fam.Id[order(-Fam.Id[,2]),]
head(Fam.Id)
```

*Distractive side note: Number one on the list, Sage family, traveled with 11 family members. An article on [Encyclopedia-Titanica](http://www.encyclopedia-titanica.org/titanic-victim/john-george-sage.html) says that John George Sage, 44, was reloacting his entire family (including his wife and nine kids) to Jacksonville, FL when they boarded Titanic. The whole family were lost in the accident. Their daughter Stella (20) was able to board a lifeboat, but gave it up when her family were unable to join.*  

Any Fam.Id with frequency lower than 3 should be assigned a generic factor to prevent overfitting.  

```{r}
Fam.IdSmall <- subset(Fam.Id, Freq < 3)
combi[combi$Fam.Id %in% Fam.IdSmall$Var1,]$Fam.Id <- "Small"
combi$Fam.Id <- as.factor(combi$Fam.Id)
```

Before we split train and test data sets, let's do two things: at instances where Age is NA, fill in the average Age; and add an additional Child factor we have used before.  

```{r}
## add average age to NAs
combiYesAge <- combi[!is.na(combi$Age),]
combi[is.na(combi$Age),"Age"] <- mean(combiYesAge$Age)
## add Child factor
combi$Child <- 0
combi[combi$Age < 15,]$Child <- 1
```

Now let's separate test and training data sets.   

```{r}
train <- combi[1:891,]
test <- combi[892:nrow(combi),]
```

#### Round 6 submission.  
Do another rpart fit with new factors.  

```{r, fig.height=4, fig.width=6}
fit <- rpart(Survived ~ Pclass + Sex + Age + Child + SibSp + Parch + Fare + Embarked
             + Title + Fam.Size + Fam.Id, 
             data=train, method="class",
             control=rpart.control(minisplit=3))
fancyRpartPlot(fit)
Prediction <- predict(fit, test, type = "class")
result5 <- data.frame(PassengerId = test$PassengerId, Survived = Prediction)
write.csv(result5, "result5.csv", row.names=F)
```

#### Round 6 submission result.  
Score improved by 0.01914, accuracy at 0.80383.  

### Random Forests  

First, before we jump into Random Forests model creation, let's use rpart decision tree to predict the age, instead of using the average age.  

```{r}
## create combi data frame from scratch
###############################
test <- read.csv("test.csv")
train <- read.csv("train.csv")
test$Survived <- NA
combi <- rbind(train, test)

combi$Name <- as.character(combi$Name)
library(stringr)
cleanTitle <- function(x) {
    nameSplit <- str_trim(strsplit(x, ",")[[1]][2])
    nameSplitClean <- strsplit(nameSplit, "\\.")[[1]][1]
    nameSplitClean
}

combi$Title <- sapply(combi$Name, cleanTitle)

combi[combi$Title %in% c("Capt", "Col", "Major", "Sir", "Dr"),]$Title <- "Sir"
combi[combi$Title %in% c("Don", "Mr"),]$Title <- "Mr"
combi[combi$Title %in% c("Dona", "Miss", "Mlle"),]$Title <- "Miss"
combi[combi$Title %in% c("Mrs", "Mme", "Ms"),]$Title <- "Mrs"
combi[combi$Title %in% c("Master", "Jonkheer"),]$Title <- "Master"
combi[combi$Title %in% c("Lady", "the Countess"),]$Title <- "Lady"

combi$Fam.Size <- combi$Parch + combi$SibSp + 1
## create a new factor that is the traveler's last name.
combi$Last.Name <- as.character(combi$Name)

getLast.Name <- function(x) {
    Last.Name <- str_trim(strsplit(x, ",")[[1]][1])
    Last.Name
}

combi$Last.Name <- sapply(combi$Name, getLast.Name)

combi$Fam.Id <- paste0(combi$Fam.Size, combi$Last.Name)
Fam.Id <- data.frame(table(combi$Fam.Id))
Fam.Id <- Fam.Id[order(-Fam.Id[,2]),]

Fam.IdSmall <- subset(Fam.Id, Freq < 3)
combi[combi$Fam.Id %in% Fam.IdSmall$Var1,]$Fam.Id <- "Small"
combi$Fam.Id <- as.factor(combi$Fam.Id)

################################
## use rpart to predict age 
hasAge <- combi[!is.na(combi$Age),]
noAge <- combi[is.na(combi$Age),]
library(rpart)
ageFit <- rpart(Age ~ Pclass + Sex + SibSp + Parch + Fare + Embarked
             + Title, 
             data=hasAge, method="anova")
combi[is.na(combi$Age),]$Age <- predict(ageFit, noAge)
## add Child factor
combi$Child <- 0
combi[combi$Age < 15,]$Child <- 1
```

Make sure there is no NAs in any factors.  

```{r}
summary(combi)
## there are two missing values in Embarked. Let's examine this factor.
combi[order(combi$Ticket),c("Ticket", "Embarked")][40:63,]
## based on the ticket number, it's almost confirmed that these two passengers embarked from S.
combi[combi$Embarked == "",]$Embarked <- "S"
## There is one passenger with Fare missing. Let's examine that.
combi[is.na(combi$Fare),]
##  examine ticket prices similar to his ticket number
ticket <- data.frame(combi)
ticket$Ticket <- as.integer(as.character(combi$Ticket))
subset(ticket, Ticket > 2800 & Ticket < 4200)
## it's reasonable to assume that his ticket price should be around $8, but let's use the median of 3rd class fare
combi$Fare[which(is.na(combi$Fare))] <- median(combi[combi$Pclass ==3 & !is.na(combi$Fare),]$Fare)
```

Because Fam.Id has too many levels, instead of using family id, let's use two factors only: small family or big family.  

```{r}
combi$Fam.Id2 <- as.character(combi$Fam.Id)
combi$Fam.Id2[combi$Fam.Id2 != "Small"] <- "Big"
combi$Fam.Id2 <- factor(combi$Fam.Id2)
```

Now it's time to create the Random Forests.  

```{r}
##install.packages("randomForest")
library(randomForest)
combi$Title <- as.factor(combi$Title)
combi$Last.Name <- as.factor(combi$Last.Name)
train <- combi[1:891,]
test <- combi[892:nrow(combi),]
test$Survived <- as.integer(0)
set.seed(12)
treeFit <- randomForest(as.factor(Survived) ~ Pclass + Sex + Age + Child + SibSp
                    + Parch + Fare + Embarked + Title + Fam.Size 
                    + Fam.Id2, data=train, importance=TRUE, ntree=2000)
varImpPlot(treeFit)

```

#### Round 7 submission  

```{r, cache=TRUE}
Prediction <- predict(treeFit, test)
result7 <- data.frame(PassengerId = test$PassengerId, Survived = Prediction)
write.csv(result7, file="result7.csv", row.names=FALSE)

```

#### Round 7 submission result  
Did not improve; accuracy dropped to 0.77512.  


#### Round 8 submission  

Use conditional Random Forest.  

```{r, cache=TRUE}
##install.packages("party")
library(party)
set.seed(12)
treeFit <- cforest(as.factor(Survived) ~ Pclass + Sex + Age + SibSp + Parch
                   + Fare + Embarked + Title + Fam.Size + Fam.Id,
                   data=train, controls=cforest_unbiased(ntree=2000, mtry=3))
Prediction <- predict(treeFit, test, OOB=TRUE, type="response")
result8 <- data.frame(PassengerId = test$PassengerId, Survived = Prediction)
write.csv(result8, file="result8.csv", row.names=FALSE)

```

#### Round 8 submission result  
Score did not improve from the best entry (0.80383).  






