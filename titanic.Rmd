---
title: 'Titanic: Machine Learning from Disaster -- a Kaggle competition entry'
author: "Sizhan Shi"
date: "November 5, 2014"
output: 
    html_document:
        theme: journal
        highlight: kate

---

## Background  
This is an entry for the kaggle machine learning competition [Titanic: Machine Learning from Disaster](https://www.kaggle.com/c/titanic-gettingStarted)

#### Introduction from kaggle's website:  
> The sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.  

> One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.  

> In this challenge, we ask you to complete the analysis of what sorts of people were likely to survive. In particular, we ask you to apply the tools of machine learning to predict which passengers survived the tragedy.  

> This Kaggle "Getting Started" Competition provides an ideal starting place for people who may not have a lot of experience in data science and machine learning. The data is highly structured, and we provide tutorials of increasing complexity for using Excel, Python, pandas in Python, and a Random Forest in Python (see links in the sidebar). We also have links to tutorials using R instead. Please use the forums freely and as much as you like. There is no such thing as a stupid question; we guarantee someone else will be wondering the same thing!  

#### Method:  
1. Load the data  
2. Perform exploratory analysis  
3. Propose hypothesis  
4. Create model reflective of the hypothesis  
5. Submit model prediction result  
6. Repeat steps 3 through 5 until reaching the limit of accuracy improvement  

#### Reference:  
http://trevorstephens.com/post/72920580937/titanic-getting-started-with-r-part-2-the

## Summary  






## Detailed analysis  

#### Load the data
```{r}
train <- read.csv("train.csv")
head(train)
## how many observations 
nrow(train)
## how many features
ncol(train)
## how many survived and their percentage
nrow(train[train$Survived==1,]) / nrow(train)
```

About 38.4% of passengers survived. Let's examine survival rate in relationship to passengers' demographic attributes. First, start with age and survival rate.  
```{r, fig.height=4, fig.width=6}

library(ggplot2)
ggplot(train, aes(as.factor(Survived),Age)) + geom_boxplot() + theme_bw()
```  

From a galance, there doens't seem to have a strong difference in age between survivors and victims. Let's look at sex next.  

```{r, fig.height=4, fig.width=6}
table(train$Sex)
table(train$Sex, train$Survived)  
##  seems like females are more likely to sruvive, let's look at the proportions:
prop.table(table(train$Sex, train$Survived), 1)
```  
74% of females survived, versus only 19% for males.  

#### Round 1 submission:  
Assume all females survived, and all males died.  

#### Round 1 submission result:  
This model had an accuracy of 0.76555.  

Now let's get back to the ages.  
```{r}
summary(train$Age)
## there are several NAs. Let's clean them up by replacing them with the average age of all passengers  
yesAge <- train[!is.na(train$Age),]
train[is.na(train$Age),"Age"] <- mean(yesAge$Age)
```  
Here, the assumption is that women and children had priority access to lifeboats and life vests. The question is how young should a child be to be considered eligible for that treatment? Let's use a for loop, starting from the age 18, and compare the difference of the survival rates between boys and girls, if we found an age group with almost equal survival rate between both genders, we have found the threadshold.  

```{r}
for (i in 2:18) {
    ageTemp <- subset(train, Age < i)
    prop.table(table(ageTemp$Sex, ageTemp$Survived),1)
    diff <- prop.table(table(ageTemp$Sex, ageTemp$Survived),1)[1,2]-prop.table(table(ageTemp$Sex, ageTemp$Survived),1)[2,2]
    print(paste("Age", i, "survival diff: ", diff))
}

```
It seems like age 15 is the threashold -- younger than 15, regardless boy or girl, the person was considered a child and given priority; older than 15, the person was considered an adult, a woman or a man, and was treated accordingly. 

```{r}
age15 <- subset(train, Age < 15)
table(age15$Sex, age15$Survived)
prop.table(table(age15$Sex, age15$Survived), 1)
```  
Children younger than 15 had a survival rate higher than 54%, much better than the average of 38%. Let's assign this level to the passengers.  

```{r}
train$Child <- 0
train[train$Age < 15,]$Child <- 1
```

#### Learnings so far:  
1. Females had higher survival rate than men;  
2. Children under the age of 15 were treated equally regardless of gender;  

Now take another look at the categorization of fare and class. First, find the most common fare price.  

```{r}
for (i in 10:100) {
    propotion <- length(train[train$Fare<i,]$Fare) / nrow(train)
    msg <- paste("Fare lower than ", i, " account for ", round(propotion, digits=2))
    print(msg)
}
```

We can categorize fare into one of the following buckets:  
1. < 10, about 38% passengers;  
2. >=10 and <15, about 13% passengers;  
3. >=15 and <24, about 10% passengers;  
4. >=24 and <28, about 11% passengers;  
5. >=28 and <52, about 10% passengers;  
6. >=52 and <78, about 8% passengers;  
7. >=78, the rest of the 10% passengers;  

Now let's assign a new class to the train dataset.  

```{r}
train$Fare2 <- "78+"
train[train$Fare<78 & train$Fare>=52,]$Fare2 <- "52-78"
train[train$Fare<52 & train$Fare>=28,]$Fare2 <- "28-52"
train[train$Fare<28 & train$Fare>=24,]$Fare2 <- "24-28"
train[train$Fare<24 & train$Fare>=15,]$Fare2 <- "15-24"
train[train$Fare<15 & train$Fare>=10,]$Fare2 <- "10-15"
train[train$Fare<10,]$Fare2 <- "<10"

```

Let's look at the distribution of passengers across gender, class, and fare level:  

```{r}
survSumFareClassSex <- aggregate(Survived ~ Fare2 + Pclass + Sex + Child, data=train, length)
colnames(survSumFareClassSex)[5] <- "Total.Size"
```

Now let's look at their survival rate.  

```{r}
survRate <- function(x) {
    round(sum(x) / length(x),digits=2)
    
}
survRateFareClassSex <- aggregate(Survived ~ Fare2 + Pclass + Sex + Child, data=train, survRate)
colnames(survRateFareClassSex)[5] <- "Survival.Rate"
survRateFareClassSexChildFull <- merge(survRateFareClassSex, survSumFareClassSex)
survRateFareClassSexChildFull[order(-survRateFareClassSexChildFull$Survival.Rate, -survRateFareClassSexChildFull$Total.Size),]
## find the females that had low survival rate 
subset(survRateFareClassSexChildFull, Sex=="female" & Survival.Rate < 0.5)
```

#### Round 2 submission:  
Seesm like females in the 3rd class with a fare price 24-28, 28-52, or 25-78 mostly died. Let's use this information to modify the gender model for round 2 submission.  

```{r}
test <- read.csv("test.csv")
## create a column "Survived"
test$Survived <- 0
test[test$Survived == 0 & test$Sex == "female",]$Survived <- 1
test[test$Sex == "female" & test$Fare >= 24 & test$Fare < 78 & test$Pclass==3,]$Survived <- 0

result2 <- subset(test, select=c(PassengerId, Survived))
write.csv(result2, "result2.csv", row.names=FALSE)
```

#### Round 2 submission result 

My score improved. Now we have 0.77990 accuracy, beat the benchmark basic random forests provied by Kaggle (0.77512).  

Based on this model, let's investigate male survivors; currently all males were assumed dead, but maybe there were characteristics indicative of their survival.  

```{r}
survRateFareClassSexChildMale <- subset(survRateFareClassSexChildFull, Sex=="male")
survRateFareClassSexChildMale[order(-survRateFareClassSexChildMale$Survival.Rate),]
```

It seems like male children, if stayed in a first or second class cabin, mostly survived.  

```{r}
subset(survRateFareClassSexChildMale, (Pclass==2 | Pclass==1) & Child==1)
```

#### Round 3 submission:  
Assume male children stayed in first or second class survived.  


```{r}
test <- read.csv("test.csv")
## create a column "Survived"
test$Survived <- 0
test[test$Survived == 0 & test$Sex == "female",]$Survived <- 1
test[test$Sex == "female" & test$Fare >= 24 & test$Fare < 78 & test$Pclass==3,]$Survived <- 0
test[test$Sex == "male" & (test$Pclass == 1 | test$Pclass == 2) & test$Age < 15 & !is.na(test$Age),]$Survived <- 1

result3 <- subset(test, select=c(PassengerId, Survived))
write.csv(result3, "result3.csv", row.names=FALSE)
```

#### Round 3 submission result:  
Score improved by 0.00478, now accuracy at 0.78469, also beat the gender, price, and class based model provided by Kaggle (0.77990).  



